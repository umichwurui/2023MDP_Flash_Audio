{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "27e17cce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27e17cce",
        "outputId": "54716bd8-73c1-4983-f476-7d4eb975ea5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_curve, auc, f1_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import random\n",
        "from moviepy.editor import *\n",
        "!pip install SpeechRecognition\n",
        "import speech_recognition as sr\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bff56af",
      "metadata": {
        "id": "7bff56af"
      },
      "source": [
        "Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c95c018",
      "metadata": {
        "id": "7c95c018"
      },
      "outputs": [],
      "source": [
        "def clean_text(t):\n",
        "    for c in string.punctuation:\n",
        "        t = t.replace(c, \" \")\n",
        "    t = t.lower()\n",
        "#     t = remove_articles(t)\n",
        "    t = t.split()\n",
        "    wordsFiltered = []\n",
        "    stops = set(stopwords.words('english'))\n",
        "    ps = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for w in t:\n",
        "        if w not in stops:\n",
        "            wordsFiltered.append(lemmatizer.lemmatize(w))\n",
        "    return ' '.join(wordsFiltered)\n",
        "\n",
        "\n",
        "\n",
        "def augment_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    if len(words) > 1:\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len, clean_text_function, augment=False):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]  # number of training examples\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "\n",
        "    for i in range(m):\n",
        "        # Clean and optionally augment the sentence\n",
        "        sentence = clean_text_function(X[i])\n",
        "        if augment:\n",
        "            sentence = augment_sentence(sentence)\n",
        "\n",
        "        sentence_words = sentence.lower().split()\n",
        "\n",
        "        j = 0\n",
        "        for w in sentence_words:\n",
        "            if w not in word_to_index or j >= max_len:\n",
        "                continue\n",
        "            X_indices[i, j] = word_to_index[w]\n",
        "            j += 1\n",
        "\n",
        "    return X_indices\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "562c2bd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "562c2bd6",
        "outputId": "263abfa0-5ef2-4d1b-c4f5-6eab377f9dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "\n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath = '/content/drive/MyDrive/FLASH MDP /audio code/glove.6B.50d.txt'\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(filepath)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a714447d",
      "metadata": {
        "id": "a714447d"
      },
      "outputs": [],
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index, non_trainable=True):\n",
        "    num_embeddings = len(word_to_index) + 1\n",
        "    embedding_dim = word_to_vec_map[\"cucumber\"].shape[0]  #  dimensionality of GloVe word vectors (= 50)\n",
        "\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (num_embeddings, embedding_dim)\n",
        "    weights_matrix = np.zeros((num_embeddings, embedding_dim))\n",
        "\n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        weights_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    embed = nn.Embedding.from_pretrained(torch.from_numpy(weights_matrix).type(torch.FloatTensor), freeze=non_trainable)\n",
        "\n",
        "    return embed, num_embeddings, embedding_dim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d68df715",
      "metadata": {
        "id": "d68df715"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attn = nn.Linear(self.hidden_dim, 1)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attn_weights = F.softmax(self.attn(lstm_output), dim=1)\n",
        "        context = torch.sum(attn_weights * lstm_output, dim=1)\n",
        "        return context, attn_weights\n",
        "class EnhancedLSTM(nn.Module):\n",
        "    def __init__(self, embedding, hidden_dim, output_dim, batch_size):\n",
        "        super(EnhancedLSTM, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = embedding\n",
        "        self.bidirectional_lstm = nn.LSTM(\n",
        "            embedding.embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=2,\n",
        "            dropout=0.5,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)  # *2 for bidirectional\n",
        "        self.attention = Attention(hidden_dim * 2)  # *2 for bidirectional\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.bidirectional_lstm(embeds)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        lstm_out, attn_weights = self.attention(lstm_out)\n",
        "        lstm_out = self.batch_norm(lstm_out)\n",
        "        out = self.fc(lstm_out)\n",
        "        return out, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7f45fc91",
      "metadata": {
        "id": "7f45fc91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8843343-6d00-4a7c-8b58-e059fc547de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription: wow this is awesome wow I love parking\n"
          ]
        }
      ],
      "source": [
        "AUDIO_FILE = \"/content/drive/MyDrive/FLASH MDP /audio code/(Audio)+Sample9_out.wav\"\n",
        "\n",
        "# use the audio file as the audio source\n",
        "text = \"\"\n",
        "r = sr.Recognizer()\n",
        "try:\n",
        "    with sr.AudioFile(AUDIO_FILE) as source:\n",
        "            audio = r.record(source)  # read the entire audio file\n",
        "            text =  r.recognize_google(audio)\n",
        "            print(\"Transcription: \" + text)\n",
        "except:\n",
        "    text = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8c1c69d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c1c69d8",
        "outputId": "22dca19e-5c3b-4b80-db3d-496ffba67035"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EnhancedLSTM(\n",
              "  (word_embeddings): Embedding(400001, 50)\n",
              "  (bidirectional_lstm): LSTM(50, 64, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (attention): Attention(\n",
              "    (attn): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "model_path = '/content/drive/MyDrive/FLASH MDP /audio code/overall_best_model (1).pth'\n",
        "\n",
        "# Assuming you have the embedding layer ready from your training\n",
        "embedding, vocab_size, embedding_dim = pretrained_embedding_layer(word_to_vec_map, word_to_index, non_trainable=True)\n",
        "\n",
        "\n",
        "hidden_dim=128\n",
        "output_size=2\n",
        "batch_size = 32\n",
        "# Create an instance of the model\n",
        "model1 = EnhancedLSTM(embedding, hidden_dim=64, output_dim=2, batch_size=64)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model1.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model1.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "486a955d",
      "metadata": {
        "id": "486a955d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58a5eec-f989-4f7d-cbc0-4f3e75140446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3809, 0.6191]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "list1 = []\n",
        "\n",
        "list1.append(text)\n",
        "X = np.array(list1)\n",
        "\n",
        "string_index = sentences_to_indices(X, word_to_index, len(list1[0].split()), clean_text)\n",
        "\n",
        "string_index_tensor = torch.tensor(string_index, dtype=torch.long)  # Convert to LongTensor\n",
        "\n",
        "\n",
        "output, _ = model1(string_index_tensor)\n",
        "output = torch.nn.functional.softmax(output, dim=1)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe9ade70",
      "metadata": {
        "id": "fe9ade70"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}