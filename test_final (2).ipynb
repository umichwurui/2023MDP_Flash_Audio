{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27e17cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import random\n",
    "from moviepy.editor import *\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "import speech_recognition as sr\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff56af",
   "metadata": {},
   "source": [
    "Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c95c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(t):\n",
    "    for c in string.punctuation:\n",
    "        t = t.replace(c, \" \")\n",
    "    t = t.lower()\n",
    "#     t = remove_articles(t)\n",
    "    t = t.split()\n",
    "    wordsFiltered = []\n",
    "    stops = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w in t:\n",
    "        if w not in stops:\n",
    "            wordsFiltered.append(lemmatizer.lemmatize(w))\n",
    "    return ' '.join(wordsFiltered)\n",
    "\n",
    "\n",
    "\n",
    "def augment_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) > 1:\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len, clean_text_function, augment=False):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]  # number of training examples\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):\n",
    "        # Clean and optionally augment the sentence\n",
    "        sentence = clean_text_function(X[i])\n",
    "        if augment:\n",
    "            sentence = augment_sentence(sentence)\n",
    "\n",
    "        sentence_words = sentence.lower().split()\n",
    "\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w not in word_to_index or j >= max_len:\n",
    "                continue\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j += 1\n",
    "    \n",
    "    return X_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "562c2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a714447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index, non_trainable=True):\n",
    "    num_embeddings = len(word_to_index) + 1                   \n",
    "    embedding_dim = word_to_vec_map[\"cucumber\"].shape[0]  #  dimensionality of GloVe word vectors (= 50)\n",
    "\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (num_embeddings, embedding_dim)\n",
    "    weights_matrix = np.zeros((num_embeddings, embedding_dim))\n",
    "\n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        weights_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    embed = nn.Embedding.from_pretrained(torch.from_numpy(weights_matrix).type(torch.FloatTensor), freeze=non_trainable)\n",
    "\n",
    "    return embed, num_embeddings, embedding_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d68df715",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedLSTM(nn.Module):\n",
    "    def __init__(self, embedding, hidden_dim, output_dim, batch_size):\n",
    "        super(EnhancedLSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = embedding\n",
    "        self.bidirectional_lstm = nn.LSTM(\n",
    "            embedding.embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=2,\n",
    "            dropout=0.5,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)  # *2 for bidirectional\n",
    "        self.attention = Attention(hidden_dim * 2)  # *2 for bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.bidirectional_lstm(embeds)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out, attn_weights = self.attention(lstm_out)\n",
    "        lstm_out = self.batch_norm(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FILE = \"Sample9.wav\"\n",
    "\n",
    "# use the audio file as the audio source      \n",
    "text = \"\"\n",
    "r = sr.Recognizer()\n",
    "try:\n",
    "    with sr.AudioFile(AUDIO_FILE) as source:\n",
    "            audio = r.record(source)  # read the entire audio file                  \n",
    "            text =  r.recognize_google(audio)\n",
    "            print(\"Transcription: \" + text)\n",
    "except:\n",
    "    text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1c69d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_embedding_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverall_best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming you have the embedding layer ready from your training\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m embedding, vocab_size, embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[43mpretrained_embedding_layer\u001b[49m(word_to_vec_map, word_to_index, non_trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      8\u001b[0m output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_embedding_layer' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = 'overall_best_model.pth'\n",
    "\n",
    "# Assuming you have the embedding layer ready from your training\n",
    "embedding, vocab_size, embedding_dim = pretrained_embedding_layer(word_to_vec_map, word_to_index, non_trainable=True)\n",
    "\n",
    "\n",
    "hidden_dim=128\n",
    "output_size=2\n",
    "batch_size = 32\n",
    "# Create an instance of the model\n",
    "model1 = EnhancedLSTM(embedding, hidden_dim=128, output_dim=2, batch_size=64)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model1.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "486a955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text,model):\n",
    "    \n",
    "    list1 = []\n",
    "\n",
    "    list1.append(text)\n",
    "    X = np.array(list1)\n",
    "    \n",
    "    string_index = sentences_to_indices(X, word_to_index, len(list1[0].split()), clean_text)\n",
    "    string_index_tensor = torch.tensor(string_index, dtype=torch.long)  # Convert to LongTensor\n",
    "    output = model1(string_index_tensor)\n",
    "    output = torch.nn.functional.softmax(output, dim=1)\n",
    "    print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f748d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9760, 0.0240]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predict(text,model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ade70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
